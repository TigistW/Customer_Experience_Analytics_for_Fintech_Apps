{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1559bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f4b0d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48102490",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load spaCy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men_core_web_sm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load your dataset\u001b[39;00m\n\u001b[32m      5\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../../data/ethiopian_bank_reviews.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\spacy\\util.py:484\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
      "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Load spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "df = pd.read_csv(\"../../data/ethiopian_bank_reviews.csv\")\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|@\\S+\", \"\", text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "df['clean_text'] = df['review'].astype(str).apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(row):\n",
    "    result = sentiment_pipeline(row['clean_text'])[0]\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    if label == 'NEGATIVE':\n",
    "        return 'negative', score\n",
    "    elif label == 'POSITIVE':\n",
    "        return 'positive', score\n",
    "    else:\n",
    "        return 'neutral', 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df[['sentiment_label', 'sentiment_score']] = df.progress_apply(lambda row: pd.Series(get_sentiment(row)), axis=1)\n",
    "\n",
    "# Save intermediate output\n",
    "df.to_csv(\"outputs/sentiment_output.csv\", index=False)\n",
    "\n",
    "# TF-IDF for Keyword Extraction\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "tfidf_scores = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "top_keywords = tfidf_scores.mean().sort_values(ascending=False).head(50)\n",
    "print(\"Top Keywords:\\n\", top_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6960a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy-based keyword extraction\n",
    "def extract_keywords_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    keywords = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha and len(token) > 3]\n",
    "    return keywords\n",
    "\n",
    "df['keywords'] = df['clean_text'].progress_apply(extract_keywords_spacy)\n",
    "\n",
    "# Manual Thematic Mapping\n",
    "theme_keywords = {\n",
    "    'Account Access Issues': ['login', 'account', 'password', 'access', 'block'],\n",
    "    'Transaction Performance': ['transfer', 'delay', 'send', 'deposit', 'payment'],\n",
    "    'User Interface & Experience': ['interface', 'design', 'layout', 'easy', 'friendly', 'ui'],\n",
    "    'Customer Support': ['support', 'help', 'call', 'response', 'wait'],\n",
    "    'Feature Requests': ['add', 'feature', 'option', 'update', 'notification']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_themes(keywords):\n",
    "    themes = []\n",
    "    for theme, kws in theme_keywords.items():\n",
    "        if any(kw in keywords for kw in kws):\n",
    "            themes.append(theme)\n",
    "    return themes if themes else ['Other']\n",
    "\n",
    "df['identified_themes'] = df['keywords'].apply(assign_themes)\n",
    "\n",
    "# Export Final Output\n",
    "df[['review', 'bank', 'rating', 'sentiment_label', 'sentiment_score', 'identified_themes']].to_csv(\n",
    "    \"outputs/task2_final_analysis.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Sentiment and thematic analysis completed and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
